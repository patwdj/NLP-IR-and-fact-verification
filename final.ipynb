{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup all imports required\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaForSequenceClassification, BertForSequenceClassification, RobertaConfig, BertConfig, RobertaTokenizer, BertTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import gensim.downloader as api\n",
    "from tqdm.auto import tqdm\n",
    "from rank_bm25 import *\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_non_alphanum, strip_multiple_whitespaces, remove_stopwords, stem_text\n",
    "import re\n",
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a2b7985",
   "metadata": {},
   "source": [
    "<b> NOTES:\n",
    "\n",
    "!! If at any point kernel crashes but a pickling point have been passed, processing up to the last pickling point can be skipped and data loaded directly instead. A sample of how to load pre-processed data and scores is available at the bottom of this notebook under \"Load Preprocessed Data and Scores\". Adjust files and variable names to as required. !! \n",
    "\n",
    "!! Action points i.e. updating variables based on the model to be tested are noted in comments and markdowns. !!\n",
    "\n",
    "</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ffd075b",
   "metadata": {},
   "source": [
    "# Part A: Evidence Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39fd58a4",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd44e8ca",
   "metadata": {},
   "source": [
    "Removal of non-alphanum characters, strip multiple white spaces, lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09103137",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_path = \"project-data/evidence.json\"\n",
    "with open(evidence_path) as evidence_file:\n",
    "    evidence_dict = json.load(evidence_file)\n",
    "\n",
    "evidence_df = pd.DataFrame.from_dict(evidence_dict, orient = \"index\")\n",
    "evidence_df.set_axis([\"evidence\"], axis = 1, inplace = True)\n",
    "\n",
    "evidence_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tqdm.pandas()\n",
    "\n",
    "def preprocess(row):\n",
    "    try:\n",
    "        sentence = row[\"evidence\"]\n",
    "    except:\n",
    "        sentence = row[\"claim_text\"]\n",
    "    CUSTOM_FILTERS = [strip_tags, strip_non_alphanum, strip_multiple_whitespaces, remove_stopwords]\n",
    "    tokens = preprocess_string(sentence, CUSTOM_FILTERS)\n",
    "    sentence = \" \".join(tokens)\n",
    "    doc = nlp(sentence)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    sentence = \" \".join(lemmatized_tokens)\n",
    "    sentence = sentence.lower()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb280b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_df['evidence_clean'] = evidence_df.progress_apply (preprocess, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d353f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_df.to_csv('./pickles/evidence_df.csv')\n",
    "with open('./pickles/evidence_df.pkl', 'wb') as f:\n",
    "    pickle.dump(evidence_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8951f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_path = \"project-data/dev-claims.json\"\n",
    "test_path = \"project-data/test-claims-unlabelled.json\"\n",
    "train_path = \"project-data/train-claims.json\"\n",
    "\n",
    "with open(dev_path) as dev_file:\n",
    "    dev_dict = json.load(dev_file)\n",
    "dev_df = pd.DataFrame.from_dict(dev_dict, orient = \"index\")\n",
    "dev_df.reset_index(inplace = True)\n",
    "\n",
    "with open(test_path) as test_file:\n",
    "    test_dict = json.load(test_file)\n",
    "test_df = pd.DataFrame.from_dict(test_dict, orient = \"index\")\n",
    "test_df.reset_index(inplace = True)\n",
    "\n",
    "with open(train_path) as train_file:\n",
    "    train_dict = json.load(train_file)\n",
    "train_df = pd.DataFrame.from_dict(train_dict, orient = \"index\")\n",
    "train_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe40b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df['clean'] = dev_df.progress_apply (preprocess, axis=1)\n",
    "test_df['clean'] = test_df.progress_apply (preprocess, axis=1)\n",
    "train_df['clean'] = train_df.progress_apply (preprocess, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be263252",
   "metadata": {},
   "source": [
    "## Get BM-25 (bm25) scores, TF-IDF (tfidf), and word2Vec (w2v) cosine similarities with all evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_tokens = evidence_df['evidence_clean'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d65c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup bm25, tfidf, and w2v\n",
    "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
    "n_components = 100 \n",
    "model_w2v = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "#get evidence vectors\n",
    "vectorizer.fit(evidence_df['evidence_clean'])\n",
    "evidence_tfidf = vectorizer.transform(evidence_df['evidence_clean'])\n",
    "bm25_vectors = BM25Okapi(evidence_tokens, b=0.3, k1=0.5)\n",
    "\n",
    "def w2vembedding(tokens, model):\n",
    "    embeddings = [model[token] for token in tokens if token in model]\n",
    "    if not embeddings:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "evidence_w2v = np.array([w2vembedding(doc, model_w2v) for doc in evidence_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b41dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data\n",
    "train_tokens = train_df['clean'].apply(lambda x: x.split())\n",
    "\n",
    "train_tfidf = vectorizer.transform(train_df['clean'])\n",
    "train_scores_tfidf = cosine_similarity(train_tfidf, evidence_tfidf)\n",
    "\n",
    "train_scores_w2v = []\n",
    "train_scores_bm25 = [] \n",
    "    \n",
    "for token in tqdm(train_tokens, desc=\"Getting bm25 and w2v scores\"):\n",
    "    w2v = w2vembedding(token, model_w2v)\n",
    "    similarity_score = cosine_similarity(w2v.reshape(1, -1), evidence_w2v)\n",
    "    train_scores_w2v.append(similarity_score[0])\n",
    "    bm25_scores = bm25_vectors.get_scores(token)                                \n",
    "    train_scores_bm25.append(bm25_scores)\n",
    "    \n",
    "train_scores_w2v = np.array(train_scores_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31920632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev data\n",
    "dev_tokens = dev_df['clean'].apply(lambda x: x.split())\n",
    "\n",
    "dev_tfidf = vectorizer.transform(dev_df['clean'])\n",
    "dev_scores_tfidf = cosine_similarity(dev_tfidf, evidence_tfidf)\n",
    "\n",
    "dev_scores_w2v = []\n",
    "dev_scores_bm25 = [] \n",
    "    \n",
    "for token in tqdm(dev_tokens, desc=\"Getting bm25 and w2v scores\"):\n",
    "    w2v = w2vembedding(token, model_w2v)\n",
    "    similarity_score = cosine_similarity(w2v.reshape(1, -1), evidence_w2v)\n",
    "    dev_scores_w2v.append(similarity_score[0])\n",
    "    bm25_scores = bm25_vectors.get_scores(token)                                \n",
    "    dev_scores_bm25.append(bm25_scores)\n",
    "\n",
    "dev_scores_w2v = np.array(dev_scores_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066980ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data\n",
    "test_tokens = test_df['clean'].apply(lambda x: x.split())\n",
    "\n",
    "test_tfidf = vectorizer.transform(test_df['clean'])\n",
    "test_scores_tfidf = cosine_similarity(test_tfidf, evidence_tfidf)\n",
    "\n",
    "test_scores_w2v = []\n",
    "test_scores_bm25 = [] \n",
    "    \n",
    "for token in tqdm(test_tokens, desc=\"Getting bm25 and w2v scores\"):\n",
    "    w2v = w2vembedding(token, model_w2v)\n",
    "    similarity_score = cosine_similarity(w2v.reshape(1, -1), evidence_w2v)\n",
    "    test_scores_w2v.append(similarity_score[0])\n",
    "    bm25_scores = bm25_vectors.get_scores(token)                                \n",
    "    test_scores_bm25.append(bm25_scores)\n",
    "    \n",
    "test_scores_w2v = np.array(test_scores_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise bm25 scores, w2v and tfidf cosine similarities are l2 normalised by default\n",
    "\n",
    "train_scores_bm25 = train_scores_bm25 / np.max(train_scores_bm25, axis=1, keepdims=True)\n",
    "dev_scores_bm25 = dev_scores_bm25 / np.max(dev_scores_bm25, axis=1, keepdims=True)\n",
    "test_scores_bm25 = test_scores_bm25 / np.max(test_scores_bm25, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f7807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to file as pickles\n",
    "with open('./pickles/train_scores_bm25.pkl', 'wb') as f:\n",
    "    pickle.dump(train_scores_bm25, f)\n",
    "with open('./pickles/dev_scores_bm25.pkl', 'wb') as f:\n",
    "    pickle.dump(dev_scores_bm25, f)\n",
    "with open('./pickles/test_scores_bm25.pkl', 'wb') as f:\n",
    "    pickle.dump(test_scores_bm25, f)\n",
    "    \n",
    "with open('./pickles/train_scores_tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(train_scores_tfidf, f)\n",
    "with open('./pickles/dev_scores_tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(dev_scores_tfidf, f)\n",
    "with open('./pickles/test_scores_tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(test_scores_tfidf, f)\n",
    "    \n",
    "with open('./pickles/train_scores_w2v.pkl', 'wb') as f:\n",
    "    pickle.dump(train_scores_w2v, f)\n",
    "with open('./pickles/dev_scores_w2v.pkl', 'wb') as f:\n",
    "    pickle.dump(dev_scores_w2v, f)\n",
    "with open('./pickles/test_scores_w2v.pkl', 'wb') as f:\n",
    "    pickle.dump(test_scores_w2v, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2eb15a5",
   "metadata": {},
   "source": [
    "## Get top-10 evidence based on calculated scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "197a72f5",
   "metadata": {},
   "source": [
    "### The following different scoring combinations are used to get top-10 evidences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "200f3c45",
   "metadata": {},
   "source": [
    "- **bm25:**  bm25 only\n",
    "- **tfidf:**  tfidf only\n",
    "- **w2v:**  word2vec only\n",
    "- **eq:**  33% bm25, 33% tfidf, 33% word2vec\n",
    "\n",
    "since bm25 performed the best independently, the following combinations are added (geared towards bm25):\n",
    "\n",
    "- **bmtf:**  50% bm25, 50% tfidf\n",
    "- **bmw2v:**  50% bm25, 50% w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eb69fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evidence_list_test(w_bm25, w_tfidf, w_w2v, n):\n",
    "    result = []\n",
    "    aggregate_scores = test_scores_bm25 * w_bm25 + test_scores_tfidf * w_tfidf + test_scores_w2v * w_w2v\n",
    "    for scores in tqdm(aggregate_scores, desc=\"Processing test claims\"):\n",
    "        sorted_indices = np.argsort(scores)\n",
    "        topn = sorted_indices[-n:][::-1]\n",
    "        result.append(topn)\n",
    "    return result\n",
    "\n",
    "def get_evidence_list_dev(w_bm25, w_tfidf, w_w2v, n):\n",
    "    result = []\n",
    "    aggregate_scores = dev_scores_bm25 * w_bm25 + dev_scores_tfidf * w_tfidf + dev_scores_w2v * w_w2v\n",
    "    for scores in tqdm(aggregate_scores, desc=\"Processing dev claims\"):\n",
    "        sorted_indices = np.argsort(scores)\n",
    "        topn = sorted_indices[-n:][::-1]\n",
    "        result.append(topn)\n",
    "    return result\n",
    "\n",
    "def get_evidence_list_train(w_bm25, w_tfidf, w_w2v, n):\n",
    "    result = []\n",
    "    aggregate_scores = train_scores_bm25 * w_bm25 + train_scores_tfidf * w_tfidf + train_scores_w2v * w_w2v \n",
    "    for scores in tqdm(aggregate_scores, desc=\"Processing train claims\"):\n",
    "        sorted_indices = np.argsort(scores)\n",
    "        topn = sorted_indices[-n:][::-1]\n",
    "        result.append(topn)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b487365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bm25 only\n",
    "\n",
    "test_list_bm25 = get_evidence_list_test(1,0,0,10)\n",
    "dev_list_bm25 = get_evidence_list_dev(1,0,0,10)\n",
    "train_list_bm25 = get_evidence_list_train(1,0,0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf only\n",
    "\n",
    "test_list_tfidf = get_evidence_list_test(0,1,0,10)\n",
    "dev_list_tfidf = get_evidence_list_dev(0,1,0,10)\n",
    "train_list_tfidf = get_evidence_list_train(0,1,0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb172e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec only\n",
    "\n",
    "test_list_w2v = get_evidence_list_test(0,0,1,10)\n",
    "dev_list_w2v = get_evidence_list_dev(0,0,1,10)\n",
    "train_list_w2v = get_evidence_list_train(0,0,1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d616aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#33% bm25, 33% tfidf, 33% word2vec\n",
    "\n",
    "test_list_eq = get_evidence_list_test(0.33,0.33,0.33,10)\n",
    "dev_list_eq = get_evidence_list_dev(0.33,0.33,0.33,10)\n",
    "train_list_eq = get_evidence_list_train(0.33,0.33,0.33,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8fd6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#50% bm25, 50% tfidf\n",
    "\n",
    "test_list_bmtf = get_evidence_list_test(0.5,0.5,0,10)\n",
    "dev_list_bmtf = get_evidence_list_dev(0.5,0.5,0,10)\n",
    "train_list_bmtf = get_evidence_list_train(0.5,0.5,0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#50% bm25, 50% w2v\n",
    "\n",
    "test_list_bmw2v = get_evidence_list_test(0.5,0,0.5,10)\n",
    "dev_list_bmw2v = get_evidence_list_dev(0.5,0,0.5,10)\n",
    "train_list_bmw2v = get_evidence_list_train(0.5,0,0.5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da23946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to file as pickles and csv\n",
    "\n",
    "dev_df['bm25'] = dev_list_bm25\n",
    "dev_df['tfidf'] = dev_list_tfidf\n",
    "dev_df['w2v'] = dev_list_w2v\n",
    "dev_df['eq'] = dev_list_eq\n",
    "dev_df['bmtf'] = dev_list_bmtf\n",
    "dev_df['bmw2v'] = dev_list_bmw2v\n",
    "\n",
    "test_df['bm25'] = test_list_bm25\n",
    "test_df['tfidf'] = test_list_tfidf\n",
    "test_df['w2v'] = test_list_w2v\n",
    "test_df['eq'] = test_list_eq\n",
    "test_df['bmtf'] = test_list_bmtf\n",
    "test_df['bmw2v'] = test_list_bmw2v\n",
    "\n",
    "train_df['bm25'] = train_list_bm25\n",
    "train_df['tfidf'] = train_list_tfidf\n",
    "train_df['w2v'] = train_list_w2v\n",
    "train_df['eq'] = train_list_eq\n",
    "train_df['bmtf'] = train_list_bmtf\n",
    "train_df['bmw2v'] = train_list_bmw2v\n",
    "\n",
    "with open('./pickles/dev_df.pkl', 'wb') as fp:\n",
    "    pickle.dump(dev_df, fp)\n",
    "with open('./pickles/test_df.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_df, fp)\n",
    "with open('./pickles/train_df.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_df, fp)\n",
    "    \n",
    "dev_df.to_csv('./csv/dev_df.csv')\n",
    "train_df.to_csv('./csv/train_df.csv')\n",
    "test_df.to_csv('./csv/test_df.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd3c77d4",
   "metadata": {},
   "source": [
    "## Save to required json format to test out evidence retrieval on dev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4719f6cd",
   "metadata": {},
   "source": [
    "We are not testing labelling at this stage. All claim-label values is saved with \"SUPPORTS\" as a placeholder. For evaluation, we will only care about Evidence Retrieval F-score (F) (i.e. Claim Classification Accuracy (A) and Harmonic Mean of F and A (H) ignored).\n",
    "\n",
    "For each combination, test out different n values of 1,4,5,6,10 and save resulting F-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c55c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evidences_json(df, evidence_list, n):\n",
    "    all_data = {}\n",
    "    for i in range(len(df.index)):\n",
    "        data = {}\n",
    "        claim_id = df.loc[i,'index']\n",
    "        data['claim_text'] = df.loc[i,'claim_text']\n",
    "        data['claim_label'] = \"SUPPORTS\"\n",
    "        evidences = []\n",
    "        for j in evidence_list[i]:\n",
    "            if len(evidences) < n:\n",
    "                evidences.append(\"evidence-\" + str(j))\n",
    "        data['evidences'] = evidences\n",
    "        all_data[claim_id] = data\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0d79fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./json/bm25.json', 'w') as f:\n",
    "    json.dump(test_evidences_json(dev_df,dev_list_bm25,10), f)\n",
    "with open('./json/tfidf.json', 'w') as f:\n",
    "    json.dump(test_evidences_json(dev_df,dev_list_tfidf,10), f)\n",
    "with open('./json/w2v.json', 'w') as f:\n",
    "    json.dump(test_evidences_json(dev_df,dev_list_w2v,10), f)\n",
    "with open('./json/eq.json', 'w') as f:\n",
    "    json.dump(test_evidences_json(dev_df,dev_list_eq,10), f)\n",
    "with open('./json/bmtf.json', 'w') as f:\n",
    "    json.dump(test_evidences_json(dev_df,dev_list_bmtf,10), f)\n",
    "with open('./json/bmw2v.json', 'w') as f:\n",
    "    json.dump(test_evidences_json(dev_df,dev_list_bmw2v,10), f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a83a148a",
   "metadata": {},
   "source": [
    "# Part B: Claim Labeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc5c8ee7",
   "metadata": {},
   "source": [
    "## B.1 Training model and variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a0a6cc5",
   "metadata": {},
   "source": [
    "### Set-up helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae0bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_int(label):\n",
    "    if label == \"SUPPORTS\":\n",
    "        return 0\n",
    "    elif label == \"REFUTES\":\n",
    "        return 1\n",
    "    elif label == \"NOT_ENOUGH_INFO\":\n",
    "        return 2\n",
    "    elif label == \"DISPUTED\":\n",
    "        return 3\n",
    "    else:\n",
    "        print(\"ERROR: invalid label\")\n",
    "\n",
    "def label_to_string(label):\n",
    "    if label == 0:\n",
    "        return \"SUPPORTS\"\n",
    "    elif label == 1:\n",
    "        return \"REFUTES\"\n",
    "    elif label == 2:\n",
    "        return \"NOT_ENOUGH_INFO\"\n",
    "    elif label == 3:\n",
    "        return \"DISPUTED\"\n",
    "    else:\n",
    "        print(\"ERROR: invalid label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eeabb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of evidence use in prediction\n",
    "K = 10\n",
    "\n",
    "#set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2ffdef0",
   "metadata": {},
   "source": [
    "### Set-up dataloaders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c973e19",
   "metadata": {},
   "source": [
    "**Train data** will be split into 80% used for training and 20% used for validation in training. The evidence used for training data will be to a total of 10, combining actual evidence, and top-10 evidence as per part A without duplicates.\n",
    "\n",
    "**Dev data** will then be used to evaluate the results of the different combinations and model. Evidence will be per part A.\n",
    "\n",
    "**Test data** will be used for codalab submission. Evidence will be per part A."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5dc3b043",
   "metadata": {},
   "source": [
    "<b> NOTE:\n",
    "\n",
    "!! Action point, update evidence retrieval combination here !!\n",
    "\n",
    "</b>\n",
    "\n",
    "The below code was run on the best combinations from part A i.e. eq and bmtf. To update to eq, update the below comb from 'bmtf' to 'eq' and run again. Depending on GPU constraints, restarting kernel may be required before re-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b43df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evidence retrieved from part A to use.\n",
    "comb = 'bmtf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_claims = []\n",
    "train_evidence = []\n",
    "train_labels = []\n",
    "\n",
    "dev_claims = []\n",
    "dev_evidence_part_a = []\n",
    "dev_labels = []\n",
    "\n",
    "for i in range(len(train_df.index)):\n",
    "    claim = train_df.loc[i, 'claim_text']\n",
    "    label = label_to_int(train_df.loc[i, 'claim_label'])\n",
    "    evidence_actual = train_df.loc[i, 'evidences']\n",
    "    evidence_part_a = train_df.loc[i, comb]\n",
    "    evidence_list = []\n",
    "    for i in evidence_actual:\n",
    "        idx = int(i.split('-')[-1])\n",
    "        evidence_list.append(evidence_df.loc[idx, 'evidence'])\n",
    "    for i in evidence_df.loc[evidence_part_a, 'evidence']:\n",
    "        if len(evidence_list) < K :\n",
    "            if i not in evidence_list:\n",
    "                evidence_list.append(i)\n",
    "        else:\n",
    "            break\n",
    "    for i in range(len(evidence_list)):\n",
    "        train_claims.append(claim)\n",
    "        train_labels.append(label)\n",
    "        train_evidence.append(evidence_list[i])\n",
    "\n",
    "for i in range(len(dev_df.index)):\n",
    "    claim = dev_df.loc[i, 'claim_text']\n",
    "    label = label_to_int(dev_df.loc[i, 'claim_label'])\n",
    "    evidence_list_part_a = dev_df.loc[i, comb]\n",
    "    for j in range(K):\n",
    "        evidence_string = evidence_df.loc[evidence_list_part_a[j], 'evidence']\n",
    "        dev_claims.append(claim)\n",
    "        dev_labels.append(label)\n",
    "        dev_evidence_part_a.append(evidence_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f392a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train data to train and validation sets.\n",
    "train_frac = 0.8\n",
    "train_evidences_train, train_evidences_val, train_labels_train, train_labels_val = train_test_split(train_evidence, train_labels, test_size=1 - train_frac, random_state=1, stratify=train_labels)\n",
    "train_claims_train, train_claims_val = train_test_split(train_claims, test_size=1 - train_frac, random_state=1, stratify=train_labels)\n",
    "\n",
    "assert(len(train_evidences_val) == len(train_labels_val) == len(train_claims_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a746c52",
   "metadata": {},
   "source": [
    "#### RoBERTa models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup parameters\n",
    "config = RobertaConfig.from_pretrained('roberta-base', num_labels=4, hidden_dropout_prob=0.3)\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', config=config, ignore_mismatched_sizes=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model.to(device)\n",
    "epochs = 3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "max_length = 512\n",
    "batch_size = 16\n",
    "\n",
    "#update model loss function to be based on train class distribution\n",
    "class_weights = [len(train_labels_train) / train_labels_train.count(i) for i in range(4)]\n",
    "class_weights = torch.tensor(class_weights).to(device)\n",
    "model.loss_fct = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc17ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimDataset(Dataset):\n",
    "    def __init__(self, claims, evidence, labels, tokenizer, max_length):\n",
    "        self.claims = claims\n",
    "        self.evidence = evidence\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claims)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        claim = self.claims[idx]\n",
    "        evidence_item = self.evidence[idx]\n",
    "        claim_and_evidence = claim + \" \" + evidence_item\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            claim_and_evidence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84961b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ClaimDataset(train_claims_train, train_evidences_train, train_labels_train, tokenizer, max_length)\n",
    "val_dataset = ClaimDataset(train_claims_val, train_evidences_val, train_labels_val, tokenizer, max_length)\n",
    "dev_dataset = ClaimDataset(dev_claims, dev_evidence_part_a, None, tokenizer, max_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update learning rate as training goes\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b72087",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    #training\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  \n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    #validation\n",
    "    model.eval()\n",
    "    progress_bar = tqdm(val_dataloader, desc=f\"Validation {epoch + 1}/{epochs}\", unit=\"batch\")\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        progress_bar.set_postfix({\"accuracy\": correct_predictions / total_predictions})\n",
    "\n",
    "    validation_accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Validation accuracy: {validation_accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b10d13b",
   "metadata": {},
   "source": [
    "#### BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7caa933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup parameters\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', num_labels=4, hidden_dropout_prob=0.3)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config, ignore_mismatched_sizes=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model.to(device)\n",
    "epochs = 3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "max_length = 512\n",
    "batch_size = 16\n",
    "\n",
    "#update model loss function to be based on train class distribution\n",
    "class_weights = [len(train_labels_train) / train_labels_train.count(i) for i in range(4)]\n",
    "class_weights = torch.tensor(class_weights).to(device)\n",
    "model.loss_fct = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimDataset(Dataset):\n",
    "    def __init__(self, claims, evidence, labels, tokenizer, max_length):\n",
    "        self.claims = claims\n",
    "        self.evidence = evidence\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claims)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        claim = self.claims[idx]\n",
    "        evidence_item = self.evidence[idx]\n",
    "        claim_and_evidence = claim + \" \" + evidence_item\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            claim_and_evidence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ClaimDataset(train_claims_train, train_evidences_train, train_labels_train, tokenizer, max_length)\n",
    "val_dataset = ClaimDataset(train_claims_val, train_evidences_val, train_labels_val, tokenizer, max_length)\n",
    "dev_dataset = ClaimDataset(dev_claims, dev_evidence_part_a, None, tokenizer, max_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bbe265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update learning rate as training goes\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7400b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    #training\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  \n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    #validation\n",
    "    model.eval()\n",
    "    progress_bar = tqdm(val_dataloader, desc=f\"Validation {epoch + 1}/{epochs}\", unit=\"batch\")\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        progress_bar.set_postfix({\"accuracy\": correct_predictions / total_predictions})\n",
    "\n",
    "    validation_accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Validation accuracy: {validation_accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55397cfc",
   "metadata": {},
   "source": [
    "## Run predictions based on the trained models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "893e5305",
   "metadata": {},
   "source": [
    "### Set-up helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45075950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions based on model\n",
    "def predict(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Predicting\", unit=\"batch\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split evidence to relevant claims i.e. to list of list\n",
    "def split_evidence(predictions, n):\n",
    "    return [predictions[i:i+n] for i in range(0, len(predictions), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e31824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#given the list of list of evidences per claim, find the majority label\n",
    "def find_majority_mjr(preds):\n",
    "    counter = Counter(preds)\n",
    "    majority, count = counter.most_common(1)[0]\n",
    "    indexes = [i for i, x in enumerate(preds) if x == majority]\n",
    "    return majority, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956fa85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_final_json(claim_id, claim_text, evidence_list, labels, num_evidence):\n",
    "    all_data = {}\n",
    "    for i in range(len(claim_id)):\n",
    "        ev_list = evidence_list[i]\n",
    "        id = claim_id[i]\n",
    "        data = {}\n",
    "        text = claim_text[i]\n",
    "        label = labels[i]\n",
    "        evidences = []\n",
    "        for j in range(len(ev_list)):\n",
    "            if j < num_evidence:\n",
    "                evidences.append(\"evidence-\" + str(ev_list[j])) \n",
    "            else:\n",
    "                break\n",
    "        data[\"claim_text\"] = text\n",
    "        data[\"claim_label\"] = label\n",
    "        data[\"evidences\"] = evidences\n",
    "        all_data[id] = data\n",
    "    return all_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3955d227",
   "metadata": {},
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00118995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dev predictions. \n",
    "dev_predictions = predict(model, dev_dataloader)\n",
    "dev_predictions = split_evidence(dev_predictions, K)\n",
    "print(\"dev predictions: \")\n",
    "print(dev_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade898f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save labels and evidence (per selecting evidence methodology)\n",
    "\n",
    "final_dev_labels = []\n",
    "final_dev_evidence_mjr = []\n",
    "final_dev_evidence_ord = []\n",
    "for i in range(len(dev_predictions)):\n",
    "    preds = dev_predictions[i]\n",
    "    evidence_ids = dev_df.loc[i, comb]\n",
    "    majority, indexes = find_majority_mjr(preds)\n",
    "    evidence_mjr = []\n",
    "    for j in indexes:\n",
    "        evidence_mjr.append(evidence_ids[j])\n",
    "    final_dev_labels.append(label_to_string(majority))\n",
    "    final_dev_evidence_mjr.append(evidence_mjr)\n",
    "    final_dev_evidence_ord.append(evidence_ids)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f2912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dev in the json format required (with actual predicted labels this time) to analyse performance\n",
    "\n",
    "dev_4_mjr_json = to_final_json(dev_df['index'],dev_df['claim_text'], final_dev_evidence_mjr, final_dev_labels, 4)\n",
    "dev_5_mjr_json = to_final_json(dev_df['index'],dev_df['claim_text'], final_dev_evidence_mjr, final_dev_labels, 5)\n",
    "dev_6_mjr_json = to_final_json(dev_df['index'],dev_df['claim_text'], final_dev_evidence_mjr, final_dev_labels, 6)\n",
    "dev_4_ord_json = to_final_json(dev_df['index'],dev_df['claim_text'], final_dev_evidence_ord, final_dev_labels, 4)\n",
    "dev_5_ord_json = to_final_json(dev_df['index'],dev_df['claim_text'], final_dev_evidence_ord, final_dev_labels, 5)\n",
    "dev_6_ord_json = to_final_json(dev_df['index'],dev_df['claim_text'], final_dev_evidence_ord, final_dev_labels, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd0015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update model name and evidence retrieval methodology in save file name\n",
    "#as required for other runs e.g. dev_bert_bmtf_4_mjr.json etc.\n",
    "\n",
    "with open('./json/dev_rbert_eq_4_mjr.json', 'w') as f:\n",
    "    json.dump(dev_4_mjr_json,f)\n",
    "with open('./json/dev_rbert_eq_5_mjr.json', 'w') as f:\n",
    "    json.dump(dev_5_mjr_json,f)\n",
    "with open('./json/dev_rbert_eq_6_mjr.json', 'w') as f:\n",
    "    json.dump(dev_6_mjr_json,f)\n",
    "with open('./json/dev_rbert_eq_4_ord.json', 'w') as f:\n",
    "    json.dump(dev_4_ord_json,f)\n",
    "with open('./json/dev_rbert_eq_5_ord.json', 'w') as f:\n",
    "    json.dump(dev_5_ord_json,f)\n",
    "with open('./json/dev_rbert_eq_6_ord.json', 'w') as f:\n",
    "    json.dump(dev_6_ord_json,f)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "151b5463",
   "metadata": {},
   "source": [
    "## B.2 Hyperparameter tuning on best performing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55122174",
   "metadata": {},
   "source": [
    "Hyperparameter tune on rbert_bmtf_4_ord. Test the following parameters:\n",
    "- batch_size = 8, learning_rate = 1e-5, epoch = 2\n",
    "- batch_size = 8, learning_rate = 1e-5, epoch = 3\n",
    "- batch_size = 8, learning_rate = 3e-5, epoch = 2\n",
    "- batch_size = 8, learning_rate = 3e-5, epoch = 3\n",
    "- batch_size = 16, learning_rate = 1e-5, epoch = 2\n",
    "- batch_size = 16, learning_rate = 1e-5, epoch = 3\n",
    "- batch_size = 16, learning_rate = 3e-5, epoch = 2\n",
    "- batch_size = 16, learning_rate = 3e-5, epoch = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0699bdb6",
   "metadata": {},
   "source": [
    "<b> NOTE:\n",
    "\n",
    "!! Action point, update batch_size, learning_rate, and epochs for hyperparameter tuning !!\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d7040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code below is basically the code from part 1. update lr, batch_size and epoch to get results.\n",
    "\n",
    "comb = 'bmtf'\n",
    "batch_size = 16\n",
    "learning_rate = 1e-5\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96dba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimDataset(Dataset):\n",
    "    def __init__(self, claims, evidence, labels, tokenizer, max_length):\n",
    "        self.claims = claims\n",
    "        self.evidence = evidence\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claims)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        claim = self.claims[idx]\n",
    "        evidence_item = self.evidence[idx]\n",
    "        claim_and_evidence = claim + \" \" + evidence_item\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            claim_and_evidence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaef0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup dataloaders\n",
    "\n",
    "train_claims = []\n",
    "train_evidence = []\n",
    "train_labels = []\n",
    "\n",
    "dev_claims = []\n",
    "dev_evidence_part_a = []\n",
    "dev_labels = []\n",
    "\n",
    "test_claims = []\n",
    "test_evidence_part_a = []\n",
    "\n",
    "for i in range(len(train_df.index)):\n",
    "    claim = train_df.loc[i, 'claim_text']\n",
    "    label = label_to_int(train_df.loc[i, 'claim_label'])\n",
    "    evidence_actual = train_df.loc[i, 'evidences']\n",
    "    evidence_part_a = train_df.loc[i, comb]\n",
    "    evidence_list = []\n",
    "    for i in evidence_actual:\n",
    "        idx = int(i.split('-')[-1])\n",
    "        evidence_list.append(evidence_df.loc[idx, 'evidence'])\n",
    "    for i in evidence_df.loc[evidence_part_a, 'evidence']:\n",
    "        if len(evidence_list) < K :\n",
    "            if i not in evidence_list:\n",
    "                evidence_list.append(i)\n",
    "        else:\n",
    "            break\n",
    "    for i in range(len(evidence_list)):\n",
    "        train_claims.append(claim)\n",
    "        train_labels.append(label)\n",
    "        train_evidence.append(evidence_list[i])\n",
    "\n",
    "for i in range(len(dev_df.index)):\n",
    "    claim = dev_df.loc[i, 'claim_text']\n",
    "    label = label_to_int(dev_df.loc[i, 'claim_label'])\n",
    "    evidence_list_part_a = dev_df.loc[i, comb]\n",
    "    for j in range(K):\n",
    "        evidence_string = evidence_df.loc[evidence_list_part_a[j], 'evidence']\n",
    "        dev_claims.append(claim)\n",
    "        dev_labels.append(label)\n",
    "        dev_evidence_part_a.append(evidence_string)\n",
    "\n",
    "for i in range(len(test_df.index)):\n",
    "    claim = test_df.loc[i, 'claim_text']\n",
    "    evidences_list_part_a = test_df.loc[i, comb]\n",
    "    for j in range(K):\n",
    "        evidence_string = evidence_df.loc[evidences_list_part_a[j], 'evidence']\n",
    "        test_claims.append(claim)\n",
    "        test_evidence_part_a.append(evidence_string)\n",
    "        \n",
    "#split train data to train and validation sets.\n",
    "train_frac = 0.8\n",
    "train_evidences_train, train_evidences_val, train_labels_train, train_labels_val = train_test_split(train_evidence, train_labels, test_size=1 - train_frac, random_state=1, stratify=train_labels)\n",
    "train_claims_train, train_claims_val = train_test_split(train_claims, test_size=1 - train_frac, random_state=1, stratify=train_labels)\n",
    "\n",
    "assert(len(train_evidences_val) == len(train_labels_val) == len(train_claims_val))\n",
    "\n",
    "#setup parameters\n",
    "config = RobertaConfig.from_pretrained('roberta-base', num_labels=4, hidden_dropout_prob=0.3)\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', config=config, ignore_mismatched_sizes=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "max_length = 512\n",
    "\n",
    "#update model loss function to be based on train class distribution\n",
    "class_weights = [len(train_labels_train) / train_labels_train.count(i) for i in range(4)]\n",
    "class_weights = torch.tensor(class_weights).to(device)\n",
    "model.loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "train_dataset = ClaimDataset(train_claims_train, train_evidences_train, train_labels_train, tokenizer, max_length)\n",
    "val_dataset = ClaimDataset(train_claims_val, train_evidences_val, train_labels_val, tokenizer, max_length)\n",
    "dev_dataset = ClaimDataset(dev_claims, dev_evidence_part_a, dev_labels, tokenizer, max_length)\n",
    "test_dataset = ClaimDataset(test_claims, test_evidence_part_a, None, tokenizer, max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "#update learning rate as training goes\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e98c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_accuracy = -1\n",
    "for epoch in range(epochs):\n",
    "    #training\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  \n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    #validation\n",
    "    model.eval()\n",
    "    progress_bar = tqdm(val_dataloader, desc=f\"Validation {epoch + 1}/{epochs}\", unit=\"batch\")\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        progress_bar.set_postfix({\"accuracy\": correct_predictions / total_predictions})\n",
    "\n",
    "    validation_accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Validation accuracy: {validation_accuracy:.4f}\")\n",
    "\n",
    "print(f\"Last validation accuracy for hyperparameter: Learning rate = {learning_rate}, Batch size = {batch_size}, Validation accuracy = {validation_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f44a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dev results and analyse\n",
    "\n",
    "dev_predictions = predict(model, dev_dataloader)\n",
    "dev_predictions = split_evidence(dev_predictions, K)\n",
    "print(\"dev predictions: \")\n",
    "print(dev_predictions)\n",
    "\n",
    "final_dev_labels = []\n",
    "final_dev_evidence_ord = []\n",
    "for i in range(len(dev_predictions)):\n",
    "    preds = dev_predictions[i]\n",
    "    evidence_ids = dev_df.loc[i, comb]\n",
    "    majority, indexes = find_majority_mjr(preds)\n",
    "    final_dev_labels.append(label_to_string(majority))\n",
    "    final_dev_evidence_ord.append(evidence_ids)\n",
    "\n",
    "dev_4_ord_json = to_final_json(dev_df['index'],dev_df['claim_text'], final_dev_evidence_ord, final_dev_labels, 4)\n",
    "\n",
    "#update file name based on batch size, learning rate, and number of epoch used \n",
    "\n",
    "with open('./json/dev_final_8_1e5_2.json', 'w') as f:\n",
    "    json.dump(dev_4_ord_json,f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d48f1af3",
   "metadata": {},
   "source": [
    "### get test prediction on final best hyperparameter tuned model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b126b9ce",
   "metadata": {},
   "source": [
    "<b> NOTE:\n",
    "\n",
    "!! Action point, make sure the last model trained is the best/final model before running the following code for test predictions !!\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup dataloaders\n",
    "\n",
    "test_claims = []\n",
    "test_evidence_part_a = []\n",
    "\n",
    "for i in range(len(test_df.index)):\n",
    "    claim = test_df.loc[i, 'claim_text']\n",
    "    evidences_list_part_a = test_df.loc[i, comb]\n",
    "    for j in range(K):\n",
    "        evidence_string = evidence_df.loc[evidences_list_part_a[j], 'evidence']\n",
    "        test_claims.append(claim)\n",
    "        test_evidence_part_a.append(evidence_string)\n",
    "        \n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "max_length = 512\n",
    "\n",
    "test_dataset = ClaimDataset(test_claims, test_evidence_part_a, None, tokenizer, max_length)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951187b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = predict(model, test_dataloader)\n",
    "test_predictions = split_evidence(test_predictions, K)\n",
    "\n",
    "final_test_labels = []\n",
    "final_test_evidence_ord = []\n",
    "\n",
    "for i in range(len(test_predictions)):\n",
    "    preds = test_predictions[i]\n",
    "    evidence_ids = test_df.loc[i, comb]\n",
    "    majority, indexes = find_majority_mjr(preds)\n",
    "    final_test_labels.append(label_to_string(majority))\n",
    "    final_test_evidence_ord.append(evidence_ids)\n",
    "\n",
    "test_4_ord_json = to_final_json(test_df['index'],test_df['claim_text'], final_test_evidence_ord, final_test_labels, 4)\n",
    "\n",
    "with open('test-claims-predictions.json', 'w') as f:\n",
    "    json.dump(test_4_ord_json,f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48e31b00",
   "metadata": {},
   "source": [
    "# Load Preprocessed Data and Scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c35ed68",
   "metadata": {},
   "source": [
    "if any pickled files need to be reloaded, just update the file path and variable name in the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a3d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./pickles/evidence_df.pkl\",'rb')\n",
    "evidence_df = pickle.load(file)\n",
    "\n",
    "file = open(\"./pickles/dev_df.pkl\",'rb')\n",
    "dev_df = pickle.load(file)\n",
    "\n",
    "file = open(\"./pickles/test_df.pkl\",'rb')\n",
    "test_df = pickle.load(file)\n",
    "\n",
    "file = open(\"./pickles/train_df.pkl\",'rb')\n",
    "train_df = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a846ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./pickles/dev_scores_bm25.pkl\",'rb')\n",
    "dev_scores_bm25 = pickle.load(file)\n",
    "\n",
    "file = open(\"./pickles/test_scores_bm25.pkl\",'rb')\n",
    "test_scores_bm25 = pickle.load(file)\n",
    "\n",
    "file = open(\"./pickles/train_scores_bm25.pkl\",'rb')\n",
    "train_scores_bm25 = pickle.load(file)\n",
    "\n",
    "file = open(\"./pickles/dev_scores_tfidf.pkl\",'rb')\n",
    "dev_scores_tfidf = pickle.load(file)\n",
    "\n",
    "file = open(\"./pickles/test_scores_tfidf.pkl\",'rb')\n",
    "test_scores_tfidf = pickle.load(file)\n",
    "\n",
    "file = open(\"./pickles/train_scores_tfidf.pkl\",'rb')\n",
    "train_scores_tfidf = pickle.load(file)\n",
    "\n",
    "file = open(\"./pickles/dev_scores_w2v.pkl\",'rb')\n",
    "dev_scores_w2v = pickle.load(file)\n",
    "\n",
    "file = open(\"./pickles/test_scores_w2v.pkl\",'rb')\n",
    "test_scores_w2v = pickle.load(file)\n",
    "\n",
    "file = open(\"./pickles/train_scores_w2v.pkl\",'rb')\n",
    "train_scores_w2v = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
